# -*- coding: utf-8 -*-
"""Capstone - Embedding path (Neumf).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qYXQ5m677Gps1fcPc4toIGXSinbBWs5P

## **Retrieve category codings for users and items**
"""

from google.colab import drive
drive.mount('/content/drive')

import pickle
with open("/content/drive/My Drive/1m_ratings.csv", 'rb') as rating_1m:
  ratings = pickle.load(rating_1m)

# Generate all_users and all_items, i.e. number of users and items 
import numpy as np
ratings['user_id'] = ratings['userId'].astype("category").cat.codes
df_sort = ratings.sort_values(by=['userId', 'timestamp']).reset_index(drop=True)
all_users = list(np.sort(df_sort.userId.unique()))
all_items = list(np.sort(df_sort.movieId.unique()))
print("Number of users: ", len(all_users))
print("Number of movies: ", len(all_items))

def train_test_split(df,n):
    """
    Splits our original data into one test and one
    training set. 
    The test set is made up of one item for each user. This is
    our holdout item used to compute Top@K later.
    The training set is the same as our original data but
    without any of the holdout items.
    Args:
        df (dataframe): Our original data
    Returns:
        df_train (dataframe): All of our data except holdout items
        df_test (dataframe): Only our holdout items.
    """

    # Create two copies of our dataframe that we can modify
    df_test = df.copy(deep=True)
    df_train = df.copy(deep=True)

    # Group by user_id and select only the last n item
    # Test dataframe
    df_test = df_test.groupby(['user_id']).tail(n)
    df_test = df_test[['userId', 'user_id', 'movieId', 'rating']]

    # Remove the test set from the test set
    mask_test = df_test.index
    df_train = df_train.drop(mask_test)
    df_train = df_train[['userId', 'user_id', 'movieId', 'rating']]

    return df_train, df_test

k = 5
df_train, df_test = train_test_split(df_sort,k)

"""Only items in the df_train are category coded, not the entire dataset, as the model is only trained on df_train dataset"""

# 110 items with cold start problem

df_train['item_id'] = df_train['movieId'].astype("category").cat.codes
df_train

item_list = df_train[['movieId', 'item_id']]
print(len(item_list))
item_list = item_list.drop_duplicates()
print(len(item_list))
item_list.head(2)

"""## **Download movies' additional information**"""

#movies = pd.read_csv("/content/drive/My Drive/movies.dat")
import pandas as pd
rnames = ['movie_id','title','genres']

movies = pd.read_table('/content/drive/My Drive/movies.dat',sep='::',header=None, names=rnames)
movies.head(2)

"""## **Grouping of movie's information**"""

movies['year'] = movies.title.str.extract("\((\d{4})\)", expand=True)
movies.year = pd.to_datetime(movies.year, format='%Y')
movies.year = movies.year.dt.year # As there are some NaN years, resulting type will be float (decimals)
movies['genre'] = movies.genres.str.split('|')
movies.head()

# Find out how many categories is each movie grouped under
genre_count = [len(i) for i in movies.genre]
genre_count_df = pd.DataFrame(genre_count, columns = ['genre_count'])
movies1 = pd.concat([movies, genre_count_df], axis = 1)
movies1['categories_movie_isin'] = movies1['genre_count'].astype(str) + "_genre_category"
movies_info = pd.merge(movies, movies1, left_on = 'movie_id', right_on = 'movie_id', how = 'left')
movies_info = pd.merge(item_list, movies1, left_on = 'movieId', right_on = 'movie_id', how = 'left')
movies_info.year = movies_info.year.astype(str)
movies_info = movies_info.drop(columns=['movie_id','genre'])
movies_info.head(3)

"""## **Read trained embeddings**"""

import pickle
with open("/content/drive/My Drive/gmf_item_embedding_neg.pickle", 'rb') as gmf_item:
  trained_gmf_items = pickle.load(gmf_item)
with open("/content/drive/My Drive/mlp_item_embeddings_neg.pickle", 'rb') as mlp_item:
  trained_mlp_items = pickle.load(mlp_item)

# Create combine data set
# Using mlp items - implicit feedback
dataset = pd.DataFrame(trained_mlp_items)
dataset['item_id'] = dataset.index
dataset = pd.merge(dataset, movies_info, left_on = 'item_id', right_on = 'item_id').dropna()
dataset['Label'] = pd.factorize(dataset['genres'])[0] # Create LabelEncoder
label_code_dict = dict(zip(dataset['Label'], dataset['genres'])) # Create dict to map LabelEncoder
print(len(dataset))
dataset.head()

# to find number of clusters with less than 5 datapoints

dataset['count'] = 1
aggnum = dataset.pivot_table(values = 'count', index = 'Label', aggfunc='sum')
aggnum_filter = aggnum[aggnum['count']<=5]
print(len(aggnum_filter))
aggnum_filter.head()

# Create combine data set
# Using gmf items - implicit feedback
dataset = pd.DataFrame(trained_gmf_items)
dataset['item_id'] = dataset.index
dataset = pd.merge(dataset, movies_info, left_on = 'item_id', right_on = 'item_id').dropna()
dataset['Label'] = pd.factorize(dataset['genres'])[0] # Create LabelEncoder
label_code_dict = dict(zip(dataset['Label'], dataset['genres'])) # Create dict to map LabelEncoder
dataset.head()

dataset.to_csv("michael_data.csv")

"""# K-Means silhouette analysis approach

## MLP silhoutte analysis
"""

# Define target variables
X = dataset.iloc[:, np.r_[0:64]]
Y = dataset['Label']
print("input:\n", X)
print("Target:\n", Y)

print(len(X), len(Y)) # confirmed that there are 3703 movies and their embeddings

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

print(__doc__)

range_n_clusters = list(range(2, 21))
b = [200, 250, 300]
range_n_clusters.extend(b)

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The silhouette coefficient can range from -1, 1 
    ax1.set_xlim([-1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters = n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-1, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=50, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

print(__doc__)

range_n_clusters = list(range(2, 21))
b = [200, 250, 300]
range_n_clusters.extend(b)

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The silhouette coefficient can range from -1, 1 
    ax1.set_xlim([-1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters = n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-1, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()

import matplotlib.pyplot as plt

xlist = list(range(5,21))
others = [200,250, 300]
xlist = xlist + others


import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize = (20,12))

my_ticks = list(range(1,20))
ax.plot(my_ticks, [0.12297846, 0.12442448, 0.1396363, 0.13865103,0.144919718,
                           0.14419718, 0.13615169, 0.13740338, 0.13745889, 0.13286498,
                           0.13330963, 0.13072298, 0.13186072, 0.13238718, 0.12974992,
                           0.13022022, 0.09938052, 0.09658163, 0.09431618])
ax.set_xticks(my_ticks)
ax.set_xticklabels(xlist)
plt.xticks(fontsize = 25)
plt.yticks(fontsize = 25)
plt.xlabel('Number of Clusters', fontsize = 28, y = 0.01)
plt.ylabel('Sihouette Score', fontsize = 28, x = 0.01)
#plt.title('Clusters with corresponding Sihouette Scores', fontsize = 25)

#plt.show()
plt.savefig('Sihouette score')

"""## GMF silhoutte analysis"""

# For GMF items embeddings -- Results not as good as MLP

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

print(__doc__)

range_n_clusters = list(range(2, 21))
b = [200, 250, 300]
range_n_clusters.extend(b)

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1
    ax1.set_xlim([-1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters = n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-1, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()

"""# DBSCAN siloutte analysis approach"""

X = dataset.iloc[:, np.r_[0:64]]
X = np.array(X)
Y = dataset['Label']
Y = np.array(Y)

# Commented out IPython magic to ensure Python compatibility.
print(__doc__)

import numpy as np

from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler


# Compute DBSCAN
db = DBSCAN(eps=7, min_samples=5).fit(X)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
print("Homogeneity: %0.3f" % metrics.homogeneity_score(Y, labels))
print("Completeness: %0.3f" % metrics.completeness_score(Y, labels))
print("V-measure: %0.3f" % metrics.v_measure_score(Y, labels))
print("Adjusted Rand Index: %0.3f"
#       % metrics.adjusted_rand_score(Y, labels))
print("Adjusted Mutual Information: %0.3f"
#       % metrics.adjusted_mutual_info_score(Y, labels))
print("Silhouette Coefficient: %0.3f"
#       % metrics.silhouette_score(X, labels))

# #############################################################################
# Plot result
import matplotlib.pyplot as plt

# Black removed and is used for noise instead.
unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]

    class_member_mask = (labels == k)

    xy = X[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=14)

    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=6)

plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()

"""# Next steps after silhoutte analysis (9 clusters)

**Summary findings of silhoutte analysis:**
- Map each movie into the 9 clusters
- K Means Silhoutte analysis
1. Using kmeans clustering to find ideal number of clusters
2. Not ideal. do not have a good optimal clustering
3. Silhouette score at 0.18 for 2 clusters, 0.14 for 9 or 10 cluster
4. Tried with both mlp and gmf: mlp works better

- DBSCAN silhoutte analysis
1. Similar to k-means, but supposed to deal with high density points
2. Not ideal as well, optimal clustering is 2 clusters, with sihoutte score at 0.58.

**Next steps:**

1. Identify the number of clusters (base on sihoutte analysis) â€”> 9
2. Find "user path" df of all users who watch a movie of the particular genre with the condition that he/she have not watched the genre in the 10 movies before the particular movie. In addition the movie sequence consist of 10 movies before and as well as 10 movies after the particular movie. 
3. Generate the following variables from the df:
- Cluster of the immediate prior movie 
- Cluster of the target movie
- Linear and non-linear distance between the two movies
- Distance between the particular movie to its nearest centroid.
- Distance between the particular movie to the immediate prior movie's centroid.

Qns to answer:
- Find whether the movie before the target movie is in the same cluster, if not, how far away is the movie?
- Hypothesis - if both movies are in the same cluster, we should see higher correlation of n to d. i.e. the closer the distance, the higher the corelation. if it is not in the same cluster, it should have lower correlation.

## Map the 9 clusters for each point
"""

# Define target variables
X = dataset.iloc[:, np.r_[0:64]]
Y = dataset['Label']
print("input:\n", X)
print("Target:\n", Y)

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

clusterer = KMeans(n_clusters = 9, random_state=10)
cluster_labels = clusterer.fit_predict(X)
cluster_labels

cluster_labels_df = pd.DataFrame(cluster_labels)
cluster_labels_df = cluster_labels_df.rename(columns = {0: 'cluster_label'})
cluster_labels_df.head(2)

dataset_cluster_label = pd.concat([dataset, cluster_labels_df], axis = 1)
dataset_cluster_label.head(2)

"""## Find the distance of each point to the nearest centroid (9 clusters)"""

from sklearn.neighbors import NearestCentroid
from sklearn.metrics import pairwise_distances_argmin_min, pairwise_distances

closest, min_dist = pairwise_distances_argmin_min(X, clusterer.cluster_centers_)
#clusterer.cluster_centers_
min_dist

# Distance of each point to the 9 clusters
X_np = np.array(X)
centroids = clusterer.cluster_centers_
distance = []
for i in X_np:
  data_point = np.array(i).reshape(1,-1)
  distance_to_point = pairwise_distances(data_point, centroids)
  distance.extend(distance_to_point)
  
distance_all_centroids_df = pd.DataFrame(distance)
distance_all_centroids_df.head()

"""## Add the distances (distance to nearest centroid and all centroids) to centroid to the dataset"""

distance_nearest_centroid = pd.DataFrame(min_dist)
distance_nearest_centroid_df = distance_nearest_centroid.rename(columns = {0: 'distance_centroid'})
distance_nearest_centroid_df.head(2)

dataset_cluster_label = pd.concat([dataset_cluster_label, distance_nearest_centroid_df], axis = 1)
dataset_cluster_label.head(2)

centroids_df = dataset_cluster_label[['item_id', 'movieId', 'title', 'genres', 'cluster_label', 'distance_centroid']]
centroids_df = pd.concat([centroids_df, distance_all_centroids_df], axis = 1)
centroids_df = centroids_df.rename(columns = {0:'cluster_0', 1:'cluster_1', 2:'cluster_2', 3:'cluster_3', 4:'cluster_4', 5:'cluster_5',
                                            6:'cluster_6', 7:'cluster_7', 8:'cluster_8'})
centroids_df.head(2)

bw = centroids_df[centroids_df['title'].str.contains('The Puppet') ]
bw

bw = centroids_df[centroids_df['title'].str.contains("Blair Witch") ]
bw

bw = centroids_df[centroids_df['title'].str.contains("Psycho") ]
bw

print(len(centroids_df)) # to confirm that total movies remain 3703

"""# Find the sequence of movies watched by users which fits the criteria ("Embedding path")

> Indented block


"""

ratings.head(2)

movies.head(2)

# Add column Genres
ratings_movies = pd.merge(ratings, movies, left_on = 'movieId', right_on = 'movie_id', how = 'left' )
ratings_movies['count'] = 1
ratings_movies = ratings_movies.sort_values(by=['userId','timestamp'])
ratings_movies['movies_order'] = ratings_movies.groupby(by=['userId'])['count'].transform(lambda x: x.cumsum())
ratings_movies['item_id'] = ratings_movies['movieId']
ratings_movies = ratings_movies.drop(['movie_id','count'], axis=1)
ratings_movies.head()

"""## Find user path"""

#type_genre = "Documentary"
#type_genre = 'Sci-Fi'
#type_genre = "Children's"
type_genre = 'Horror'

all_genres = ratings_movies[['movieId', 'title', 'genres']]
all_genres = all_genres.drop_duplicates()
all_genres['genre'] = all_genres.genres.str.split('|')
sub_df = all_genres[all_genres.genres.str.contains(type_genre)]
print(len(sub_df))
print(sub_df.head(2))

from sklearn.metrics.pairwise import manhattan_distances

n_front = 10
n_back = 10

all_users_targetmovies = pd.DataFrame()
selected_movies = list(set(sub_df['movieId']))

for target in selected_movies:

  # chosee only instances where the selected movies has at least 'n_front' movies before them (improve computational effiency)
  filter_movies = ratings_movies[(ratings_movies['movieId'] == target) & (ratings_movies['movies_order'] > n_front) ] # Selected movies 
  movies_name = list(ratings_movies[ratings_movies['movieId'] == target]['title'].drop_duplicates())[0]

  # to ignore blank dataframes
  if len(filter_movies) != 0:
  
    # Check when user watch selected movies
    order_dict = {}
    for i in list(filter_movies.index.values):
      order_dict[filter_movies['userId'][i]] = filter_movies['movies_order'][i]

    seq_df = pd.DataFrame()
    for k in list(order_dict.keys()):
      df_temp = ratings_movies.loc[(ratings_movies['userId'] == k) & ((order_dict[k] - n_front) <= ratings_movies['movies_order']) & ((order_dict[k] + n_back) >= ratings_movies['movies_order'])]
      seq_df = pd.concat([seq_df, df_temp])

    seq_df['count'] = 1
    seq_df['new_movies_order'] = seq_df.groupby(by = ['userId'])['count'].transform(lambda x: x.cumsum())

    genres_seq_first = seq_df.groupby('userId')['genre'].apply(lambda x: x.tolist()) # Get sequence of genres by user
 
    user_path_df = pd.DataFrame()
    for user in list(genres_seq_first.index.values):
      check_previous = 0
      num_movies = 0
      for j in genres_seq_first[user][:n_front]: # check previous movies not in selected genre
        if type_genre not in j:
          check_previous += 1
      if check_previous == n_front: #and num_movies >= n_after: 
        user_path_df = pd.concat([user_path_df, seq_df[seq_df['userId'] == user][['userId','movieId','title','genre','timestamp', 'new_movies_order']]])
  
    # Next steps - using the user_path_df of the target movie, find all users' previous movie cluster and target movie cluster and distance
    
    if len(user_path_df) != 0: 
      all_user_path_df = pd.merge(user_path_df, dataset_cluster_label, how = 'left', on = 'movieId')

      na_users = all_user_path_df[all_user_path_df.isnull().any(axis=1)].userId
      all_users = all_user_path_df.userId.unique()
      filtered_users = set(all_users) - set(na_users)

      stepping_movies = []
      previousmovie_cluster_label_list = []
      targetmovie_cluster_label_list = []
      linear_distance_list = []
      nonlinear_distance_list = []
      num_movies = []
      users_list = []
    

      for user_targetmovie in filtered_users:
        df = all_user_path_df[all_user_path_df['userId'] == user_targetmovie]
        movie_title = df.iloc[n_front: n_front +1,]['title_y'].values.tolist()
        previous_movie_label = df.iloc[n_front -1:n_front,]['cluster_label'].values.tolist()
        target_movie_label = df.iloc[n_front:n_front+1,]['cluster_label'].values.tolist()
        #previous_movie_label = df.iloc[n_front:n_front+1,]['distance_centroid'].values.tolist()
        #target_movie_distance_centroid = df.iloc[n_front:n_front+1,]['distance_centroid'].values.tolist()

        users_list.append(user_targetmovie)
        stepping_movies.extend(movie_title)
        previousmovie_cluster_label_list.extend(previous_movie_label)
        targetmovie_cluster_label_list.extend(target_movie_label)

        # find distance
        target_movie_location = df[df['userId']== user_targetmovie][n_front: n_front + 1][np.r_[0:64]].to_numpy() 
        previous_movie_location = df[df['userId'] == user_targetmovie][n_front-1 : n_front][np.r_[0:64]].to_numpy()
        linear_distance_list.append(np.linalg.norm(target_movie_location - previous_movie_location))
        nonlinear_distance_list.extend(manhattan_distances(target_movie_location, previous_movie_location).ravel())

        genres_seq = df.groupby('userId')['genre'].apply(lambda x: x.tolist()) # Get sequence of genres by user

        count = 0
        for genres_list in genres_seq[user_targetmovie][n_front:]: 
          if type_genre in genres_list:
            count += 1
        num_movies.append(count)

      final_df = pd.DataFrame({'user': users_list, 'target_movie_cluster': targetmovie_cluster_label_list, 'previous_movie_cluster':previousmovie_cluster_label_list,
                             'target_movie_title':stepping_movies, 'linear_distance': linear_distance_list, 'nonlinear_distance': nonlinear_distance_list, 'movies_count': num_movies})
    
      all_users_targetmovies = pd.concat([all_users_targetmovies, final_df])
  

all_users_targetmovies

"""## Include distance variables (i.e. to centroids and prior movie) into df"""

# just a validation check on the above
new_df = all_users_targetmovies.drop_duplicates()
len(new_df)

def centroid_dis(row):
  if row['previous_movie_cluster'] == 0 :
      col_str = 'cluster_' + str(0)
      return row[col_str]
  elif row['previous_movie_cluster'] == 1 :
      col_str = 'cluster_' + str(1)
      return row[col_str]
  elif row['previous_movie_cluster'] == 2 :
      col_str = 'cluster_' + str(2)
      return row[col_str]
  elif row['previous_movie_cluster'] == 3 :
      col_str = 'cluster_' + str(3)
      return row[col_str]
  elif row['previous_movie_cluster'] == 4 :
      col_str = 'cluster_' + str(4)
      return row[col_str]
  elif row['previous_movie_cluster'] == 5 :
      col_str = 'cluster_' + str(5)
      return row[col_str]
  elif row['previous_movie_cluster'] == 6 :
      col_str = 'cluster_' + str(6)
      return row[col_str]
  elif row['previous_movie_cluster'] == 7 :
      col_str = 'cluster_' + str(7)
      return row[col_str]
  elif row['previous_movie_cluster'] == 8 :
      col_str = 'cluster_' + str(8)
      return row[col_str]

users_in_genre.head()
centroids_dis.head()
filter_users
final_analysis1.head()

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

fig, ax3 = plt.subplots( figsize=(15,8))

final_analysis1 = final_analysis1[final_analysis1['user_count'] >= 50] # pvalue
filter_users['avg_movies'] = filter_users['movies_count'] / filter_users['user_count']
corr_centroid, pvalue_centroid = pearsonr(filter_users['distance_centroid'], filter_users['avg_movies'])
p1 = sns.regplot(filter_users['distance_centroid'], filter_users['avg_movies'], ax = ax3, order = 1, logx = False)

plt.suptitle("Movies count vs other variables - Documentary's genre")
ax3.set_title("Dist to centroid vs total movie_count \n corr: %f  pvalue: %f" % (corr_centroid, pvalue_centroid))
plt.show()

users_in_genre.head()
analysis.head()

new_df2 = pd.merge(new_df, centroids_df, how = 'left', left_on = 'target_movie_title', right_on = 'title')
print(len(new_df2))

# use the above condition function to filter out the corresponding movie_cluster
new_df2['distance_lastmovie_centroid'] = new_df2.apply (lambda row: centroid_dis(row), axis=1)

new_df2['user_count'] = 1
analysis = new_df2[['user', 'target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'linear_distance', 'nonlinear_distance', 'movies_count', 'user_count',
                   'distance_centroid', 'distance_lastmovie_centroid']]
analysis.head()

# generate total movies count and users counts of the combined key : "target movie / target movie's cluster / prior movie's cluster"
users_in_genre = analysis.pivot_table(values = ['movies_count','user_count'], index = ['target_movie_title', 'target_movie_cluster'],
               aggfunc = {'movies_count': np.sum, 'user_count' : np.sum}).reset_index()
print(len(users_in_genre))
users_in_genre.head()

centroids_dis = analysis[['target_movie_cluster', 'target_movie_title', 'distance_centroid']]
centroids_dis = centroids_dis.drop_duplicates()
centroids_dis.head()

# find correlation between linear distance vs movies counts and non-linear distance vs movie counts of the combined key: "target movie / target movie's cluster / prior movie's cluster"
testing = analysis[['target_movie_cluster', 'target_movie_title', 'linear_distance', 'movies_count', 'nonlinear_distance']]
testing_corr = testing.groupby(['target_movie_cluster', 'target_movie_title']).corr().reset_index()
print(len(testing_corr))
testing_corr = testing_corr[testing_corr['level_2'] == 'movies_count'].drop(['movies_count'], axis = 1)
print(len(testing_corr))
testing_corr.head()

final_analysis1 = pd.merge(users_in_genre, centroids_dis, how = 'left', on = ['target_movie_title', 'target_movie_cluster'])
final_analysis2 = pd.merge(final_analysis1, testing_corr, how = 'left', on = ['target_movie_title', 'target_movie_cluster'])
final_analysis2 = final_analysis2.rename(columns = {'linear_distance': 'corr_linear_distance', 'non-linear_distance': 'corr_nonlinear_distance'})
filter_users = final_analysis2[final_analysis2['user_count'] >= 50] # pvalue
print(len(filter_users))
filter_users.head()

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

fig, (ax3, ax4) = plt.subplots(1, 2, figsize=(15,8))

corr_centroid, pvalue_centroid = pearsonr(filter_users['distance_centroid'], filter_users['movies_count'])
#corr_priorcentroid__, pvalue_priorcentroid = pearsonr(filter_users['distance_lastmovie_centroid'], filter_users['movies_count'])

p1 = sns.regplot(filter_users['distance_centroid'], filter_users['movies_count'], ax = ax3, order = 1, logx = False)
#p2 = sns.regplot(filter_users['distance_lastmovie_centroid'], filter_users['movies_count'], ax = ax4)

plt.suptitle("Movies count vs other variables - Documentary's genre")
ax3.set_title("Dist to centroid vs total movie_count \n corr: %f  pvalue: %f" % (corr_centroid, pvalue_centroid))
#ax4.set_title("Dist previous movie's centroid vs total movie_count - \n corr: %f  pvalue: %f" % (corr_priorcentroid__, pvalue_priorcentroid))
plt.show()
fig.savefig('%s.png' %type_genre)

new_df2 = pd.merge(new_df, centroids_df, how = 'left', left_on = 'target_movie_title', right_on = 'title')
print(len(new_df2))

# use the above condition function to filter out the corresponding movie_cluster
new_df2['distance_lastmovie_centroid'] = new_df2.apply (lambda row: centroid_dis(row), axis=1)

new_df2['user_count'] = 1
analysis = new_df2[['user', 'target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'linear_distance', 'nonlinear_distance', 'movies_count', 'user_count',
                   'distance_centroid', 'distance_lastmovie_centroid']]
analysis.head()

# generate total movies count and users counts of the combined key : "target movie / target movie's cluster / prior movie's cluster"
users_in_genre = analysis.pivot_table(values = ['movies_count','user_count'], index = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'],
               aggfunc = {'movies_count': np.sum, 'user_count' : np.sum}).reset_index()
print(len(users_in_genre))
users_in_genre.head()

centroids_dis = analysis[['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'distance_centroid', 'distance_lastmovie_centroid']]
centroids_dis = centroids_dis.drop_duplicates()
centroids_dis.head()

# find correlation between linear distance vs movies counts and non-linear distance vs movie counts of the combined key: "target movie / target movie's cluster / prior movie's cluster"
testing = analysis[['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'linear_distance', 'movies_count', 'nonlinear_distance']]
testing_corr = testing.groupby(['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title']).corr().reset_index()
print(len(testing_corr))
testing_corr = testing_corr[testing_corr['level_3'] == 'movies_count'].drop(['movies_count'], axis = 1)
print(len(testing_corr))
testing_corr.head()

print(len(users_in_genre), len(centroids_dis), len(testing_corr))

final_analysis1 = pd.merge(users_in_genre, centroids_dis, how = 'left', on = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'])
final_analysis2 = pd.merge(final_analysis1, testing_corr, how = 'left', on = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'])
final_analysis2 = final_analysis2.rename(columns = {'linear_distance': 'corr_linear_distance', 'non-linear_distance': 'corr_nonlinear_distance'})
filter_users = final_analysis2[final_analysis2['user_count'] >= 50] # pvalue
filter_users['avg_movies'] = filter_users['movies_count'] / filter_users['user_count']
filter_users.head()

"""## Plot the distance of target movie from its centroid against total movies watched"""

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

fig, (ax3, ax4) = plt.subplots(1, 2, figsize=(15,8))

corr_centroid, pvalue_centroid = pearsonr(filter_users['distance_centroid'], filter_users['avg_movies'])
corr_priorcentroid__, pvalue_priorcentroid = pearsonr(filter_users['distance_lastmovie_centroid'], filter_users['avg_movies'])

p1 = sns.regplot(filter_users['distance_centroid'], filter_users['avg_movies'], ax = ax3, order = 1, logx = False)
p2 = sns.regplot(filter_users['distance_lastmovie_centroid'], filter_users['avg_movies'], ax = ax4)

plt.suptitle("Movies count vs other variables - Documentary's genre")
ax3.set_title("Dist to centroid vs total movie_count \n corr: %f  pvalue: %f" % (corr_centroid, pvalue_centroid))
ax4.set_title("Dist previous movie's centroid vs total movie_count - \n corr: %f  pvalue: %f" % (corr_priorcentroid__, pvalue_priorcentroid))
plt.show()
fig.savefig('%s.png' %type_genre)

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

fig, (ax3, ax4) = plt.subplots(1, 2, figsize=(15,8))

corr_centroid, pvalue_centroid = pearsonr(filter_users['distance_centroid'], filter_users['movies_count'])
corr_priorcentroid__, pvalue_priorcentroid = pearsonr(filter_users['distance_lastmovie_centroid'], filter_users['movies_count'])

p1 = sns.regplot(filter_users['distance_centroid'], filter_users['movies_count'], ax = ax3, order = 1, logx = False)
p2 = sns.regplot(filter_users['distance_lastmovie_centroid'], filter_users['movies_count'], ax = ax4)

plt.suptitle("Movies count vs other variables - Documentary's genre")
ax3.set_title("Dist to centroid vs total movie_count \n corr: %f  pvalue: %f" % (corr_centroid, pvalue_centroid))
ax4.set_title("Dist previous movie's centroid vs total movie_count - \n corr: %f  pvalue: %f" % (corr_priorcentroid__, pvalue_priorcentroid))
plt.show()
fig.savefig('%s.png' %type_genre)

"""## Tabulate results of correlation of (distance between target and prior movie) against movies watched"""

all_users_targetmovies.head()

all_users_targetmovies['user_count'] = 1
table_users = pd.pivot_table(all_users_targetmovies, values = 'user_count', index = 'target_movie_cluster', 
                             columns = 'previous_movie_cluster', aggfunc = np.sum)
table_users

from scipy.stats import pearsonr
target_movies_list = all_users_targetmovies.target_movie_cluster.values.tolist()
target_movies_list = set(target_movies_list)
target_movies_list

previous_movies_list = all_users_targetmovies.previous_movie_cluster.values.tolist()
previous_movies_list = set(previous_movies_list)

target = []
prior = []
lin_correlation = []
nonlin_correlation = []
for i in target_movies_list:
  for j in previous_movies_list:
    df = all_users_targetmovies[(all_users_targetmovies['target_movie_cluster'] == i) & (all_users_targetmovies['previous_movie_cluster'] == j)]
    linear_d = df.linear_distance.values.tolist()
    non_linear_d = df.nonlinear_distance.values.tolist()
    num_movies = df.movies_count.values.tolist()
    if len(linear_d) <= 1:
      lin_correlation.append(0)
      nonlin_correlation.append(0)
    else:
      linear_d_corr, __ = pearsonr(linear_d, num_movies)
      nonlinear_d_corr, __ = pearsonr(non_linear_d, num_movies)
      lin_correlation.append(linear_d_corr)
      nonlin_correlation.append(nonlinear_d_corr)

    target.append(i)
    prior.append(j)
    
test = pd.DataFrame({'target': target, 'prior': prior, 'lin_corr': lin_correlation, 'nonlin_corr': nonlin_correlation})
test

table_linear_corr = pd.pivot_table(test, values = 'lin_corr', index = 'target', columns = 'prior')
table_linear_corr

table_nonlinear_corr = pd.pivot_table(test, values = 'nonlin_corr', index = 'target', columns = 'prior')
table_nonlinear_corr

"""# Find top 3 genres of each cluster"""

df = dataset_cluster_label[['item_id', 'movieId', 'genres', 'cluster_label']]
df['genre'] = df.genres.str.split('|')
df1 = df.explode('genre')
df1.head()

df1['count'] = 1
df_agg = df1.groupby(by = ['cluster_label', 'genre']).sum()
g = df_agg['count'].groupby(level=0, group_keys=False)
final_analysis = g.nlargest(3)
final_analysis

"""# Correlation plot
**- Loop for all genres, all users**
"""

x = all_genres.genre.values.tolist()
y = []
for i in x:
  for k in i:
    y.append(k)
unique_genres = list(set(y))
unique_genres

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import manhattan_distances

n_front = 10
n_back = 10

all_genres = ratings_movies[['movieId', 'title', 'genres']]
all_genres = all_genres.drop_duplicates()
all_genres['genre'] = all_genres.genres.str.split('|')

all_genres_df = pd.DataFrame()

for type_genre in unique_genres:

  sub_df = all_genres[all_genres.genres.str.contains(type_genre)]

  all_users_targetmovies = pd.DataFrame()
  selected_movies = list(set(sub_df['movieId']))
  print('genre: ', type_genre, ',    number of movies with selected genre: ', len(selected_movies))

  for target in selected_movies:

    # chosee only instances where the selected movies has at least 'n_front' movies before them (improve computational effiency)
    filter_movies = ratings_movies[(ratings_movies['movieId'] == target) & (ratings_movies['movies_order'] > n_front) ] # Selected movies 
    movies_name = list(ratings_movies[ratings_movies['movieId'] == target]['title'].drop_duplicates())[0]

    # to ignore blank dataframes
    if len(filter_movies) != 0:
  
      # Check when user watch selected movies
      order_dict = {}
      for i in list(filter_movies.index.values):
        order_dict[filter_movies['userId'][i]] = filter_movies['movies_order'][i]

      seq_df = pd.DataFrame()
      for k in list(order_dict.keys()):
        df_temp = ratings_movies.loc[(ratings_movies['userId'] == k) & ((order_dict[k] - n_front) <= ratings_movies['movies_order']) & ((order_dict[k] + n_back) >= ratings_movies['movies_order'])]
        seq_df = pd.concat([seq_df, df_temp])

      seq_df['count'] = 1
      seq_df['new_movies_order'] = seq_df.groupby(by = ['userId'])['count'].transform(lambda x: x.cumsum())

      genres_seq_first = seq_df.groupby('userId')['genre'].apply(lambda x: x.tolist()) # Get sequence of genres by user
 
      user_path_df = pd.DataFrame()
      for user in list(genres_seq_first.index.values):
        check_previous = 0
        num_movies = 0
        for j in genres_seq_first[user][:n_front]: # check previous movies not in selected genre
          if type_genre not in j:
            check_previous += 1
        if check_previous == n_front: #and num_movies >= n_after: 
          user_path_df = pd.concat([user_path_df, seq_df[seq_df['userId'] == user][['userId','movieId','title','genre','timestamp', 'new_movies_order']]])
  
      # Next steps - using the user_path_df of the target movie, find all users' previous movie cluster and target movie cluster and distance
    
      if len(user_path_df) != 0: 
        all_user_path_df = pd.merge(user_path_df, dataset_cluster_label, how = 'left', on = 'movieId')

        na_users = all_user_path_df[all_user_path_df.isnull().any(axis=1)].userId
        all_users = all_user_path_df.userId.unique()
        filtered_users = set(all_users) - set(na_users)

        stepping_movies = []
        previousmovie_cluster_label_list = []
        targetmovie_cluster_label_list = []
        linear_distance_list = []
        nonlinear_distance_list = []
        num_movies = []
        users_list = []
    

        for user_targetmovie in filtered_users:
          df = all_user_path_df[all_user_path_df['userId'] == user_targetmovie]
          movie_title = df.iloc[n_front: n_front +1,]['title_y'].values.tolist()
          previous_movie_label = df.iloc[n_front -1:n_front,]['cluster_label'].values.tolist()
          target_movie_label = df.iloc[n_front:n_front+1,]['cluster_label'].values.tolist()
          #previous_movie_label = df.iloc[n_front:n_front+1,]['distance_centroid'].values.tolist()
          #target_movie_distance_centroid = df.iloc[n_front:n_front+1,]['distance_centroid'].values.tolist()

          users_list.append(user_targetmovie)
          stepping_movies.extend(movie_title)
          previousmovie_cluster_label_list.extend(previous_movie_label)
          targetmovie_cluster_label_list.extend(target_movie_label)

          # find distance
          target_movie_location = df[df['userId']== user_targetmovie][n_front: n_front + 1][np.r_[0:64]].to_numpy() 
          previous_movie_location = df[df['userId'] == user_targetmovie][n_front-1 : n_front][np.r_[0:64]].to_numpy()
          linear_distance_list.append(np.linalg.norm(target_movie_location - previous_movie_location))
          nonlinear_distance_list.extend(manhattan_distances(target_movie_location, previous_movie_location).ravel())

          genres_seq = df.groupby('userId')['genre'].apply(lambda x: x.tolist()) # Get sequence of genres by user

          count = 0
          for genres_list in genres_seq[user_targetmovie][n_front:]: 
            if type_genre in genres_list:
              count += 1
          num_movies.append(count)

        final_df = pd.DataFrame({'user': users_list, 'target_movie_cluster': targetmovie_cluster_label_list, 'previous_movie_cluster':previousmovie_cluster_label_list,
                                 'target_movie_title':stepping_movies, 'linear_distance': linear_distance_list, 'nonlinear_distance': nonlinear_distance_list, 'movies_count': num_movies})
    
        all_users_targetmovies = pd.concat([all_users_targetmovies, final_df])

  new_df = all_users_targetmovies.drop_duplicates()

  print('all users: ', len(all_users_targetmovies), ' validation check on no duplicates: ', len(new_df))

  new_df2 = pd.merge(new_df, centroids_df, how = 'left', left_on = 'target_movie_title', right_on = 'title')

  # use the above condition to filter out the corresponding movie_cluster
  new_df2['distance_lastmovie_centroid'] = new_df2.apply (lambda row: centroid_dis(row), axis=1)
  new_df2['user_count'] = 1
  analysis = new_df2[['user', 'target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'linear_distance', 'nonlinear_distance', 'movies_count', 'user_count',
                            'distance_centroid', 'distance_lastmovie_centroid']]
  users_in_genre = analysis.pivot_table(values = ['movies_count','user_count'], index = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'],
                                              aggfunc = {'movies_count': np.sum, 'user_count' : np.sum}).reset_index()
        
  centroids_dis = analysis[['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'distance_centroid', 'distance_lastmovie_centroid']]
  centroids_dis = centroids_dis.drop_duplicates()

  testing = analysis[['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'linear_distance', 'movies_count', 'nonlinear_distance']]
  testing_corr = testing.groupby(['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title']).corr().reset_index()
  testing_corr = testing_corr[testing_corr['level_3'] == 'movies_count'].drop(['movies_count'], axis = 1)

  final_analysis1 = pd.merge(testing_corr, users_in_genre, how = 'left', on = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'])
  final_analysis2 = pd.merge(final_analysis1, centroids_dis, how = 'left', on = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'])
  filter_users = final_analysis2[final_analysis2['user_count'] >= 50] # pvalue
  filter_users['avg_movies'] = filter_users['movies_count'] / filter_users['user_count']
  filter_users['genre'] = type_genre 
  print('final df with number of datapoints: ', len(filter_users))

  all_genres_df = pd.concat([all_genres_df, filter_users])

len(all_genres_df)

all_genres_df.to_csv("finaldf.csv")

import pickle
with open('all_genres_df.pickle', 'wb') as finaldf:
    pickle.dump(all_genres_df, finaldf)

all_genres_df

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import manhattan_distances

n_front = 10
n_back = 10

all_genres = ratings_movies[['movieId', 'title', 'genres']]
all_genres = all_genres.drop_duplicates()
all_genres['genre'] = all_genres.genres.str.split('|')

for type_genre in unique_genres:

  sub_df = all_genres[all_genres.genres.str.contains(type_genre)]

  all_users_targetmovies = pd.DataFrame()
  selected_movies = list(set(sub_df['movieId']))
  print('genre: ', type_genre, ',    number of movies with selected genre: ', len(selected_movies))

  all_users_targetmovies = pd.DataFrame()

  for target in selected_movies:

    # chosee only instances where the selected movies has at least 'n_front' movies before them (improve computational effiency)
    filter_movies = ratings_movies[(ratings_movies['movieId'] == target) & (ratings_movies['movies_order'] > n_front) ] # Selected movies 
    movies_name = list(ratings_movies[ratings_movies['movieId'] == target]['title'].drop_duplicates())[0]

    # to ignore blank dataframes
    if len(filter_movies) != 0:
  
      # Check when user watch selected movies
      order_dict = {}
      for i in list(filter_movies.index.values):
        order_dict[filter_movies['userId'][i]] = filter_movies['movies_order'][i]

      seq_df = pd.DataFrame()
      for k in list(order_dict.keys()):
        df_temp = ratings_movies.loc[(ratings_movies['userId'] == k) & ((order_dict[k] - n_front) <= ratings_movies['movies_order']) & ((order_dict[k] + n_back) >= ratings_movies['movies_order'])]
        seq_df = pd.concat([seq_df, df_temp])

      seq_df['count'] = 1
      seq_df['new_movies_order'] = seq_df.groupby(by = ['userId'])['count'].transform(lambda x: x.cumsum())

      genres_seq_first = seq_df.groupby('userId')['genre'].apply(lambda x: x.tolist()) # Get sequence of genres by user
 
      user_path_df = pd.DataFrame()
      for user in list(genres_seq_first.index.values):
        check_previous = 0
        num_movies = 0
        for j in genres_seq_first[user][:n_front]: # check previous movies not in selected genre
          if type_genre not in j:
            check_previous += 1
        if check_previous == n_front: #and num_movies >= n_after: 
          user_path_df = pd.concat([user_path_df, seq_df[seq_df['userId'] == user][['userId','movieId','title','genre','timestamp', 'new_movies_order']]])
  
      # Next steps - using the user_path_df of the target movie, find all users' previous movie cluster and target movie cluster and distance
    
      if len(user_path_df) != 0: 
        all_user_path_df = pd.merge(user_path_df, dataset_cluster_label, how = 'left', on = 'movieId')

        na_users = all_user_path_df[all_user_path_df.isnull().any(axis=1)].userId
        all_users = all_user_path_df.userId.unique()
        filtered_users = set(all_users) - set(na_users)

        stepping_movies = []
        previousmovie_cluster_label_list = []
        targetmovie_cluster_label_list = []
        linear_distance_list = []
        nonlinear_distance_list = []
        num_movies = []
        users_list = []
    

        for user_targetmovie in filtered_users:
          df = all_user_path_df[all_user_path_df['userId'] == user_targetmovie]
          movie_title = df.iloc[n_front: n_front +1,]['title_y'].values.tolist()
          previous_movie_label = df.iloc[n_front -1:n_front,]['cluster_label'].values.tolist()
          target_movie_label = df.iloc[n_front:n_front+1,]['cluster_label'].values.tolist()
          #previous_movie_label = df.iloc[n_front:n_front+1,]['distance_centroid'].values.tolist()
          #target_movie_distance_centroid = df.iloc[n_front:n_front+1,]['distance_centroid'].values.tolist()

          users_list.append(user_targetmovie)
          stepping_movies.extend(movie_title)
          previousmovie_cluster_label_list.extend(previous_movie_label)
          targetmovie_cluster_label_list.extend(target_movie_label)

          # find distance
          target_movie_location = df[df['userId']== user_targetmovie][n_front: n_front + 1][np.r_[0:64]].to_numpy() 
          previous_movie_location = df[df['userId'] == user_targetmovie][n_front-1 : n_front][np.r_[0:64]].to_numpy()
          linear_distance_list.append(np.linalg.norm(target_movie_location - previous_movie_location))
          nonlinear_distance_list.extend(manhattan_distances(target_movie_location, previous_movie_location).ravel())

          genres_seq = df.groupby('userId')['genre'].apply(lambda x: x.tolist()) # Get sequence of genres by user

          count = 0
          for genres_list in genres_seq[user_targetmovie][n_front:]: 
            if type_genre in genres_list:
              count += 1
          num_movies.append(count)

        final_df = pd.DataFrame({'user': users_list, 'target_movie_cluster': targetmovie_cluster_label_list, 'previous_movie_cluster':previousmovie_cluster_label_list,
                                 'target_movie_title':stepping_movies, 'linear_distance': linear_distance_list, 'nonlinear_distance': nonlinear_distance_list, 'movies_count': num_movies})
    
        all_users_targetmovies = pd.concat([all_users_targetmovies, final_df])

  new_df = all_users_targetmovies.drop_duplicates()

  print('all users: ', len(all_users_targetmovies), ' validation check on no duplicates: ', len(new_df))

  new_df2 = pd.merge(new_df, centroids_df, how = 'left', left_on = 'target_movie_title', right_on = 'title')

  # use the above condition to filter out the corresponding movie_cluster
  new_df2['distance_lastmovie_centroid'] = new_df2.apply (lambda row: centroid_dis(row), axis=1)
  new_df2['user_count'] = 1
  analysis = new_df2[['user', 'target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'linear_distance', 'nonlinear_distance', 'movies_count', 'user_count',
                            'distance_centroid', 'distance_lastmovie_centroid']]
  users_in_genre = analysis.pivot_table(values = ['movies_count','user_count'], index = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'],
                                              aggfunc = {'movies_count': np.sum, 'user_count' : np.sum}).reset_index()
        
  centroids_dis = analysis[['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'distance_centroid', 'distance_lastmovie_centroid']]
  centroids_dis = centroids_dis.drop_duplicates()

  testing = analysis[['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'linear_distance', 'movies_count', 'nonlinear_distance']]
  testing_corr = testing.groupby(['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title']).corr().reset_index()
  testing_corr = testing_corr[testing_corr['level_3'] == 'movies_count'].drop(['movies_count'], axis = 1)

  final_analysis1 = pd.merge(testing_corr, users_in_genre, how = 'left', on = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'])
  final_analysis2 = pd.merge(final_analysis1, centroids_dis, how = 'left', on = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'])
  filter_users = final_analysis2[final_analysis2['user_count'] >= 50] # pvalue
  filter_users['avg_movies'] = filter_users['movies_count'] / filter_users['user_count']
  print('final df with number of datapoints: ', len(filter_users))

  if len(filter_users) >= 2:
    fig, (ax3, ax4) = plt.subplots(1, 2, figsize=(15,8))

    corr_centroid, pvalue_centroid = pearsonr(filter_users['distance_centroid'], filter_users['avg_movies'])
    corr_priorcentroid__, pvalue_priorcentroid = pearsonr(filter_users['distance_lastmovie_centroid'], filter_users['avg_movies'])

    sns.regplot(filter_users['distance_centroid'], filter_users['avg_movies'], ax = ax3, order = 1, logx = False)
    sns.regplot(filter_users['distance_lastmovie_centroid'], filter_users['avg_movies'], ax = ax4)

    plt.suptitle("Movies count vs other variables - %s genre" %type_genre)
    ax3.set_title("Dist to centroid vs total movie_count \n corr: %f  pvalue: %f" % (corr_centroid, pvalue_centroid))
    ax4.set_title("Dist previous movie's centroid vs total movie_count - \n corr: %f  pvalue: %f" % (corr_priorcentroid__, pvalue_priorcentroid))
    plt.show()
    fig.savefig('%s.png' %type_genre)
        
  else:
    print('%s has less than 2 datapoints' %type_genre, '\n')

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import manhattan_distances

n_front = 10
n_back = 10

all_genres = ratings_movies[['movieId', 'title', 'genres']]
all_genres = all_genres.drop_duplicates()
all_genres['genre'] = all_genres.genres.str.split('|')

for type_genre in unique_genres:

  sub_df = all_genres[all_genres.genres.str.contains(type_genre)]

  all_users_targetmovies = pd.DataFrame()
  selected_movies = list(set(sub_df['movieId']))
  print('genre: ', type_genre, ',    number of movies with selected genre: ', len(selected_movies))

  all_users_targetmovies = pd.DataFrame()

  for target in selected_movies:

    # chosee only instances where the selected movies has at least 'n_front' movies before them (improve computational effiency)
    filter_movies = ratings_movies[(ratings_movies['movieId'] == target) & (ratings_movies['movies_order'] > n_front) ] # Selected movies 
    movies_name = list(ratings_movies[ratings_movies['movieId'] == target]['title'].drop_duplicates())[0]

    # to ignore blank dataframes
    if len(filter_movies) != 0:
  
      # Check when user watch selected movies
      order_dict = {}
      for i in list(filter_movies.index.values):
        order_dict[filter_movies['userId'][i]] = filter_movies['movies_order'][i]

      seq_df = pd.DataFrame()
      for k in list(order_dict.keys()):
        df_temp = ratings_movies.loc[(ratings_movies['userId'] == k) & ((order_dict[k] - n_front) <= ratings_movies['movies_order']) & ((order_dict[k] + n_back) >= ratings_movies['movies_order'])]
        seq_df = pd.concat([seq_df, df_temp])

      seq_df['count'] = 1
      seq_df['new_movies_order'] = seq_df.groupby(by = ['userId'])['count'].transform(lambda x: x.cumsum())

      genres_seq_first = seq_df.groupby('userId')['genre'].apply(lambda x: x.tolist()) # Get sequence of genres by user
 
      user_path_df = pd.DataFrame()
      for user in list(genres_seq_first.index.values):
        check_previous = 0
        num_movies = 0
        for j in genres_seq_first[user][:n_front]: # check previous movies not in selected genre
          if type_genre not in j:
            check_previous += 1
        if check_previous == n_front: #and num_movies >= n_after: 
          user_path_df = pd.concat([user_path_df, seq_df[seq_df['userId'] == user][['userId','movieId','title','genre','timestamp', 'new_movies_order']]])
  
      # Next steps - using the user_path_df of the target movie, find all users' previous movie cluster and target movie cluster and distance
    
      if len(user_path_df) != 0: 
        all_user_path_df = pd.merge(user_path_df, dataset_cluster_label, how = 'left', on = 'movieId')

        na_users = all_user_path_df[all_user_path_df.isnull().any(axis=1)].userId
        all_users = all_user_path_df.userId.unique()
        filtered_users = set(all_users) - set(na_users)

        stepping_movies = []
        previousmovie_cluster_label_list = []
        targetmovie_cluster_label_list = []
        linear_distance_list = []
        nonlinear_distance_list = []
        num_movies = []
        users_list = []
    

        for user_targetmovie in filtered_users:
          df = all_user_path_df[all_user_path_df['userId'] == user_targetmovie]
          movie_title = df.iloc[n_front: n_front +1,]['title_y'].values.tolist()
          previous_movie_label = df.iloc[n_front -1:n_front,]['cluster_label'].values.tolist()
          target_movie_label = df.iloc[n_front:n_front+1,]['cluster_label'].values.tolist()
          #previous_movie_label = df.iloc[n_front:n_front+1,]['distance_centroid'].values.tolist()
          #target_movie_distance_centroid = df.iloc[n_front:n_front+1,]['distance_centroid'].values.tolist()

          users_list.append(user_targetmovie)
          stepping_movies.extend(movie_title)
          previousmovie_cluster_label_list.extend(previous_movie_label)
          targetmovie_cluster_label_list.extend(target_movie_label)

          # find distance
          target_movie_location = df[df['userId']== user_targetmovie][n_front: n_front + 1][np.r_[0:64]].to_numpy() 
          previous_movie_location = df[df['userId'] == user_targetmovie][n_front-1 : n_front][np.r_[0:64]].to_numpy()
          linear_distance_list.append(np.linalg.norm(target_movie_location - previous_movie_location))
          nonlinear_distance_list.extend(manhattan_distances(target_movie_location, previous_movie_location).ravel())

          genres_seq = df.groupby('userId')['genre'].apply(lambda x: x.tolist()) # Get sequence of genres by user

          count = 0
          for genres_list in genres_seq[user_targetmovie][n_front:]: 
            if type_genre in genres_list:
              count += 1
          num_movies.append(count)

        final_df = pd.DataFrame({'user': users_list, 'target_movie_cluster': targetmovie_cluster_label_list, 'previous_movie_cluster':previousmovie_cluster_label_list,
                                 'target_movie_title':stepping_movies, 'linear_distance': linear_distance_list, 'nonlinear_distance': nonlinear_distance_list, 'movies_count': num_movies})
    
        all_users_targetmovies = pd.concat([all_users_targetmovies, final_df])

  new_df = all_users_targetmovies.drop_duplicates()

  print('all users: ', len(all_users_targetmovies), ' validation check on no duplicates: ', len(new_df))

  new_df2 = pd.merge(new_df, centroids_df, how = 'left', left_on = 'target_movie_title', right_on = 'title')

  # use the above condition to filter out the corresponding movie_cluster
  new_df2['distance_lastmovie_centroid'] = new_df2.apply (lambda row: centroid_dis(row), axis=1)
  new_df2['user_count'] = 1
  analysis = new_df2[['user', 'target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'linear_distance', 'nonlinear_distance', 'movies_count', 'user_count',
                            'distance_centroid', 'distance_lastmovie_centroid']]
  users_in_genre = analysis.pivot_table(values = ['movies_count','user_count'], index = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'],
                                              aggfunc = {'movies_count': np.sum, 'user_count' : np.sum}).reset_index()
        
  centroids_dis = analysis[['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'distance_centroid', 'distance_lastmovie_centroid']]
  centroids_dis = centroids_dis.drop_duplicates()




  testing = analysis[['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title', 'linear_distance', 'movies_count', 'nonlinear_distance']]
  testing_corr = testing.groupby(['target_movie_cluster', 'previous_movie_cluster', 'target_movie_title']).corr().reset_index()
  testing_corr = testing_corr[testing_corr['level_3'] == 'movies_count'].drop(['movies_count'], axis = 1)

  final_analysis1 = pd.merge(testing_corr, users_in_genre, how = 'left', on = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'])
  final_analysis2 = pd.merge(final_analysis1, centroids_dis, how = 'left', on = ['target_movie_title', 'target_movie_cluster', 'previous_movie_cluster'])
  filter_users = final_analysis2[final_analysis2['user_count'] >= 50] # pvalue
  print('final df with number of datapoints: ', len(filter_users))

  if len(filter_users) >= 2:
    fig, (ax3, ax4) = plt.subplots(1, 2, figsize=(15,8))

    corr_centroid, pvalue_centroid = pearsonr(filter_users['distance_centroid'], filter_users['movies_count'])
    corr_priorcentroid__, pvalue_priorcentroid = pearsonr(filter_users['distance_lastmovie_centroid'], filter_users['movies_count'])

    sns.regplot(filter_users['distance_centroid'], filter_users['movies_count'], ax = ax3, order = 1, logx = False)
    sns.regplot(filter_users['distance_lastmovie_centroid'], filter_users['movies_count'], ax = ax4)

    plt.suptitle("Movies count vs other variables - %s genre" %type_genre)
    ax3.set_title("Dist to centroid vs total movie_count \n corr: %f  pvalue: %f" % (corr_centroid, pvalue_centroid))
    ax4.set_title("Dist previous movie's centroid vs total movie_count - \n corr: %f  pvalue: %f" % (corr_priorcentroid__, pvalue_priorcentroid))
    plt.show()
    fig.savefig('%s.png' %type_genre)
        
  else:
    print('%s has less than 2 datapoints' %type_genre, '\n')

"""# Plots"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("finaldf.csv")
df = df.drop(['Unnamed: 0'], axis = 1, )
df.head()



bw = df[df['target_movie_title'].str.contains('Blair Witch')]
users_bw = bw['user_count'].values.tolist()
users_bw

print(sum(i for i in users_bw))
bw

from scipy import stats

slope, intercept, r_value, p_value, std_err = stats.linregress(x = df['distance_centroid'], y = df['avg_movies'])
print(slope, intercept, r_value, p_value, std_err)

import seaborn as sns
from scipy.stats import pearsonr

corr_centroid, pvalue_centroid = pearsonr(df['distance_centroid'], df['avg_movies'])
#corr_priorcentroid__, pvalue_priorcentroid = pearsonr(filter_users['distance_lastmovie_centroid'], filter_users['avg_movies'])

fig, ax = plt.subplots(figsize = (15,10))

sns.regplot( x = df['distance_centroid'], y = df['avg_movies'], scatter_kws={"color": "blue"}, line_kws={"color": "red"})
plt.title("All genres", fontsize = 25, x = 0.1, y = 0.92)
plt.xlabel('Distance to centroid', fontsize = 25)
plt.ylabel('Movies per user', fontsize = 25)
plt.xticks(fontsize = 25)
plt.yticks(fontsize = 25)
#plt.set_title("Avg movies/user vs distance to centroid \n corr: %f  pvalue: %f" % (corr_centroid, pvalue_centroid))
#ax4.set_title("Dist previous movie's centroid vs total movie_count - \n corr: %f  pvalue: %f" % (corr_priorcentroid__, pvalue_priorcentroid))
plt.savefig("all movies(per user).png")

subdf = df[(df['genre'] == 'Horror') & (df['distance_centroid'] > 3.9) & (df['distance_centroid'] < 4)]
subdf

subdf = df[df['genre'] == 'Horror']
fig, ax = plt.subplots(figsize = (15,10))

sns.regplot( x = subdf['distance_centroid'], y = subdf['avg_movies'], scatter_kws={"color": "blue"}, line_kws={"color": "red"})
ax.axvline(linewidth = 2, color='g', x = 3.935, linestyle = '--')
plt.title("Horror", fontsize = 25, x = 0.1, y = 0.92)
plt.xlabel('Distance to centroid', fontsize = 25)
plt.ylabel('Movies per user', fontsize = 25)
plt.xticks(fontsize = 25)
plt.yticks(fontsize = 25)
ax.annotate("Cluster 4", xy=(3.935, 2.5), xytext=(3.6, 2.8), fontsize = 25,
            arrowprops=dict(arrowstyle="->"))
ax.annotate("Cluster 1", xy=(3.935, 2.39), xytext=(3.6, 2.2), fontsize = 25,
            arrowprops=dict(arrowstyle="->"))
ax.annotate("Cluster 2", xy=(3.935, 1.51), xytext=(3.6, 1.3), fontsize = 25,
            arrowprops=dict(arrowstyle="->"))

#ax.annotate("", xy=(0.5, 0.5), xytext=(0, 0),
#            arrowprops=dict(arrowstyle="->"))

plt.savefig("Horror example.png")

import seaborn as sns
subdf = df[df['genre'] == 'Horror']
fig, ax = plt.subplots(figsize = (15,10))

sns.regplot( x = subdf['distance_centroid'], y = subdf['avg_movies'], scatter_kws={"color": "blue"}, line_kws={"color": "red"})
ax.axvline(linewidth = 2, color='g', x = 3.075, linestyle = '--')
plt.title("Horror", fontsize = 25, x = 0.1, y = 0.92)
plt.xlabel('Distance to centroid', fontsize = 25)
plt.ylabel('Movies per user', fontsize = 25)
plt.xticks(fontsize = 25)
plt.yticks(fontsize = 25)
ax.annotate("Cluster 4", xy=(3.935, 2.5), xytext=(3.6, 2.8), fontsize = 25,
            arrowprops=dict(arrowstyle="->"))
ax.annotate("Cluster 1", xy=(3.935, 2.39), xytext=(3.6, 2.2), fontsize = 25,
            arrowprops=dict(arrowstyle="->"))
ax.annotate("Cluster 2", xy=(3.935, 1.51), xytext=(3.6, 1.3), fontsize = 25,
            arrowprops=dict(arrowstyle="->"))

#ax.annotate("", xy=(0.5, 0.5), xytext=(0, 0),
#            arrowprops=dict(arrowstyle="->"))

plt.savefig("Horror example.png")

subdf = df[(df['genre'] == 'Horror') & (df['distance_centroid'] > 3.06) & (df['distance_centroid'] < 3.08)]
subdf

subdf = df[df['genre'] == 'Horror']
from scipy import stats

slope, intercept, r_value, p_value, std_err = stats.linregress(x = subdf['distance_centroid'], y = subdf['avg_movies'])
print(slope, intercept, r_value, p_value, std_err)

subdf = df[df['genre'] == 'Action']
fig, ax = plt.subplots(figsize = (15,10))

sns.regplot( x = subdf['distance_centroid'], y = subdf['avg_movies'], scatter_kws={"color": "blue"}, line_kws={"color": "red"})
ax.axvline(linewidth = 2, color='g', x = 4.515, linestyle = '--')
plt.title("Action", fontsize = 20, x = 0.1, y = 0.95)
plt.xlabel('Distance to centroid', fontsize = 'xx-large')
plt.ylabel('Movies per user', fontsize = 'xx-large')
plt.xticks(fontsize = 20)
plt.yticks(fontsize = 20)
plt.savefig("all movies(per user).png")

genres_lst = df.genre.tolist()
genres_lst = list(set(genres_lst))
#genres_lst1 = genres_lst.sort()
genres_lst.sort()

genres_lst

fig, axs = plt.subplots(nrows = 6, ncols = 3, figsize=(25, 40), sharex=True, sharey=False)
fig.text(0.5, 0.1, 'Distance from centroid', ha='center', fontsize = 30)
fig.text(0.06, 0.5, 'Total movies watched thereafter', va='center', rotation='vertical', fontsize = 30)

axs = axs.ravel()

for i in range(18):

    #axs[i].sns.regplot(df['distance_centroid'], df['avg_movies'], ax = ax1, order = 1, logx = False)
    subdf = df[df['genre'] == genres_lst[i]]
    sns.regplot( x = subdf['distance_centroid'], y = subdf['movies_count'], ax = axs[i], 
                scatter_kws={"color": "blue"}, line_kws={"color": "red"})
    #fig.suptitle(genres_lst[i], y = 0.98)
    axs[i].set_title(genres_lst[i], y = 0.85, x = 0.30, fontsize = 25)
    axs[i].set(xlabel=None)
    axs[i].set(ylabel=None)
    axs[i].tick_params(labelsize = 25)
    axs[i].set_xticks(ticks = [2,3,4,5])
    axs[i].set_xticklabels(labels = [2,3,4,5], fontsize = 25)
    #axs[i].set(y_ticks(fontsize = 20))

fig.savefig('All plots.png' )